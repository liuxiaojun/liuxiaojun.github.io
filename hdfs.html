<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
HDFS是用于运行在大量普通商用机集群上的一整套文件存储系统。这样做主要是基于成本考虑。对于使用者来说，大量普通商用机可以节省成本，无需购置特别的服务器，同时也节省了大量的后期维护费用。对于普通的商用机来说，虽然在单独使用中出现问题的概率不大，但是一旦被应用于大规模集群运算，总体出现问题的概率还是不低。因此，HDFS在设计之初，就根据此种需求设计出了一种优雅地处理硬件错误的方式，例如持续硬件监控、灾难恢复、错误预处理以及数据备份等。通过这种处理，大大降低了硬件之间的耦合关系，给大规范集群的配置创建了一个更加友好和便于操作的环境。
 
对于HDFS存储的对象，也就是数据的存储，HDFS开创性的设计出一套文件存储方式，即对文件进行分割后分别存放。HDFS天生就是为大规模数据存储与计算服务的，将大文件进行分隔，分割后存放在既定的存储快中，并通过预先设定的优化处理模式对存储的数据进行预处理，从而解决了大文件存储与计算的需求。对于大部分的文件来说，一旦文件生成完毕，更多的是对文件进行读取而非频繁的修改。对于普通文件的读取操作来说，HDFS一般主要分为两种--大规模的持续性读取与小型化随机读取。针对这两种读取方式，HDFS分别采取了不同的对应策略。

首先对于大规模的数据读取，HDFS采用的是在存储时进行优化，也就是在文件写入HDFS系统的时候，就对体积较大的文件采用分割后集中式存储的方式，使得未来的读取能够在一个文件的连续区域中进行，从而节省寻址及复制时间。

而对小数据的读取，HDFS更多的做法是吧小规模的随机读取操作进行合并后对读取顺序进行排序，这样可以在一定程度上按序读取，提高读取效率。因此可以说，HDFS更多地考虑对数据读取的批处理，而不是对单独命令的执行。

对于保证协调性来说，HDFS使用多种设计确保并提供了系统的灵活性，例如强调工作的协同与并发性，放松对一致性模型的要求等；还引入了写入锁的机制，对多个写入操作要求采用原子性，从而保证多用户在对同一文件进行写入操作时能够获得正确的写入行为。

对于HDFS架构来说，一个HDFS集群包括两大部分，即NameNode与DataNode。这样分配的主要作用是将管理与计算分离。一般来说，一个集群中会有一个NameNode与若干个DataNode共同工作。NameNode是集群的主服务器，主要是用于对HDFS中所有的文件及内容数据进行维护，并不断读取记录集群中DataNode主机情况与工作状态，并通过读取与写入镜像日志文件的方式进行存储。而DataNode在HDFS集群中担任任务具体执行角色，是集群的工作节点。文件被分成若干个大小相同的数据块，分别存储在若干个DataNode上，DataNode会定期向集群内NameNode发送自己的运行状态与存储内容，并根据NameNode发送的指令进行工作。NameNode负责接受客户端发送过来的信息，然后将文件存储位置信息发送给提交请求的客户端，由客户端直接与DataNode进行联系，从而进行部分文件的运算与操作。

HDFS使用Block对文件的存储进行操作。对于传统磁盘来说，磁盘都有默认的存储单元，通常使用的是数据定义中的最小存储单元。Block是HDFS的基本存储单元，默认大小是84M，这个大小远远大于一般文件系统的默认存储大小。这样做的好处有很多。第一，使用较大的存储块，即使将存储文件分隔在不同的存储介质中，也可以大大减少用户与节点之间的通信需求。一般的情况下，客户端与数据存储段可能分散在不同位置，彼此之间通过通信协议进行交流，较大的存储区域可以较少用户为获取数据存放位置信息而需要占用的资源，并且加大的存储块也方便用户更多次连续地进行读写操作。第二，采用较大尺寸的存储块，方便HDFS将更多的基本信息存放在节点内存中，这些基本信息包括文件名、存放的位置、存放地点等。

一般性文件的基本信息加载在内存之中，可以更好滴维护NameNode与DataNode之间的通信。

除此之外，采用Block对文件星星存储大大提供了文件灾难生存与恢复能力。HDFS还可以对已经存储的Block进行多副本备份，将每个Block至少复制3个相互独立的硬件上。这样做的好处是确保在发生硬件故障的时候，能够迅速地从其他意见中读取相应的文件数据。而具体复制到多少个独立硬件上也是可以设置的。

文件通过NameNode在系统中进行分配，DataNode负责对文件进行存储，并将本节点信息反馈给NameNode，从而完成文件存储的分配以及存储状态的获取。
HDFS是一种高度分布的多层结构布局。典型的HDFS集群是将数百个服务器同时部署在同一个集群之内，并且可以同时被来自同一或者不同地点的多个客户机访问。HDFS存储的目标有两个，即最大化数据可靠性与可用性以及最大化利用网络带宽资源。为了实现这两个目的，仅仅在多台机器上进行单一存储是不够的，不能预防硬件设备失误带来的损失。因此采用多副本存储复制，在多个节点之间重复分布存储数据副本，保证了即使有一个存储所需数据硬件设备损坏，也能够安全合理的获得所需要的数据。同时为扩大资源的利用带宽，对节点的选择则需要进行专门的研究与设计。

那么，NameNode如何选择具体的DataNode来存储数据，并能够充分利用现有的宽带那？在回答这个问题之前，首先需要对节点和宽带作出HDFS中的定义。在通过宽带网络进行数据交换与处理过程中，吧数据传输速度作为节点之间的衡量标准是自然并且可行的。但是问题在于，在实际工作中，宽带或者传输速率的影响的因素非常多，例如网络的稳定性、其他数据的传输宽带占用以及交换机的负载等等都能够对某个时间点的宽带起到决定性的作用。因此，并不能简单地吧传输速度作为节点距离的衡量标准。为了解决通过测量宽带不能够获得一个长期、准确、稳定速率的问题，HDFS采用了另一个简单的方法对宽带进行衡量。假设客户端即数据的获取点，根据数据存放的相对位置来衡量距离值。设定如下，距离值依次增加。
* 统一节点上的存储数据
* 同一机架上不同节点上的存储数据
* 同一数据中心不同机架上的存储数据
* 不同数据中心的节点

在确定数据距离值后，HDFS中NameNode节点即可根据定义的距离值对需要存储的数据进行切割和存储。同时，为了维护数据的稳定性与预防灾难，还需要将数据进行副本的备份操作。副本存放位置的选取是需要进行考虑和评价的。复制操作要对宽带及传输速率进行衡量，衡量的标准也是上文定义的距离值。如果想要要求更高的网络速度，那么将所有数据都存放在同一节点上市最为合适的，但是此种存放方法非常危险。如果你无法正确估量和测试硬件的稳定状态，一旦灾难发生，则可能无法避免数据损失。如果换另一种极端的方法考虑，将数据存放在不同数据中心不同节点中，那么，不同数据中心不同几点同时发生故障的可能性是非常低的。但是，由于客户端有可能需要多多个节点中的数据进行读写操作，因此，此种存放方法会在传输效率上大打折扣。

HDFS数据存放策略就是采用同节点与同机架并行的存储模式，制作成三个副本存储在运行客户端的当前节点上存放在第一个副本，第二个副本存在与第一个副本同一个机架上而非同一个节点，第三个副本放置的位置在另外一个机架上。

在HDFS上面，数据之间的交互是通过宽带网络进行的，而对于具体的交互程序，又分为输入和输出过程。

输入过程如下，客户端或者用户通过调用FileSystem对象的open()方法打开需要读取的文件，从而创建一个HDFS的读取对象实例。FileSystem通过远程协议调用NameNode获取文件处于前端的几个Block位置。并根据上下文定义客户端与DataNode距离值进行排序。NameNode将于客户端对应最近的那个文件起始Block地址返回给客户端，客户端根据地址创建一个FSDataInputStream开始对数据进行读取。FSDataInputStream根据返回的Block地址，链接到最近的DataNode上对数据Block开水读取，通过反复调用read()方法，以流的方式从DataNode中读取数据，当读到Block结尾的时候FSDataInputStream会关闭当前DataNode的链接，然后查找能够读取下一个Block的最好的DataNode。这些操作对客户端是透明的，客户端感觉到的是连续的流，也就是说读取的时候就开始查找下一个块所在的地址。读取完成，调用close方法，关闭FSDataInputStream。客户端在读取数据的过程中，只需要访问NameNode一次获得起始位置的Block，即可开始对数据进行持续而直接的读取。这样做的好处是HDFS能够服务大量的客户端。不同的数据传输是通过集群中不同的DataNode与客户端直接连接而成的。NameNode只需要提供一次查询基本信息的操作即可，而不需要提供任何数据服务工作，极大提高了HDFS的整体性能。

当客户端的读取操作发生错误是，客户端回想NameNode报告错误，并氢气球NameNode排除错误的DataNode后重新根据距离值排序，从而获得一个新的DataNode的读取路径。如果所有的DataNode都报告读取失败，那么任务读取失败。NameNode中存放的基本信息称为元数据，一般包括数据类型，大小，格式以及数据的存放形式。

与HDFS的输入类似，HDFS的输出也需要通过NameNode获得DataNode的基本信息，从而执行对应的输出过程。首先客户端通过调用create方法来向FileSystem请求创建文件。FileSystem向NameNode发出创建请求后，NameNode进行了若干步检查来确定需要创建的文件或目录是否已经存在，并且确认提出请求的客户端是否有权限创建相应的文件或目录。如果检查通过，NameNode会保存创建文件的请求并向客户端返回一个FSDataOutStream地址供客户端读出数据。而如果在NameNode中的检查没通过，则客户端会立即受到一个IOException，表示文件创建请求失败，任务被停止。客户端在受到FSDataOutputStream后调用write方法对数据进行输出操作。客户端会将数据分成若干个Block，然后在NameNode分配的FSDataOutputStream中到到具有最近距离的DataNode进行数据的输出。

对数据进行备份操作在数据被写到第一个DataNode完毕后立即开始，如果默认将数据进行三次副本备份的话，那么首选将数据写到第一个DataNode中。此后由第一个DataNode将数据复制到第二个DataNode，第二个DataNode会将数据写到第三个DataNode。全部数据写出结束后，FsDataOutputStream调用close方法，关闭写出流。
<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p>HDFS是用于运行在大量普通商用机集群上的一整套文件存储系统。这样做主要是基于成本考虑。对于使用者来说，大量普通商用机可以节省成本，无需购置特别的服务器，同时也节省了大量的后期维护费用。对于普通的商用机来说，虽然在单独使用中出现问题的概率不大，但是一旦被应用于大规模集群运算，总体出现问题的概率还是不低。因此，HDFS在设计之初，就根据此种需求设计出了一种优雅地处理硬件错误的方式，例如持续硬件监控、灾难恢复、错误预处理以及数据备份等。通过这种处理，大大降低了硬件之间的耦合关系，给大规范集群的配置创建了一个更加友好和便于操作的环境。</p>

<p>对于HDFS存储的对象，也就是数据的存储，HDFS开创性的设计出一套文件存储方式，即对文件进行分割后分别存放。HDFS天生就是为大规模数据存储与计算服务的，将大文件进行分隔，分割后存放在既定的存储快中，并通过预先设定的优化处理模式对存储的数据进行预处理，从而解决了大文件存储与计算的需求。对于大部分的文件来说，一旦文件生成完毕，更多的是对文件进行读取而非频繁的修改。对于普通文件的读取操作来说，HDFS一般主要分为两种--大规模的持续性读取与小型化随机读取。针对这两种读取方式，HDFS分别采取了不同的对应策略。</p>

<p>首先对于大规模的数据读取，HDFS采用的是在存储时进行优化，也就是在文件写入HDFS系统的时候，就对体积较大的文件采用分割后集中式存储的方式，使得未来的读取能够在一个文件的连续区域中进行，从而节省寻址及复制时间。</p>

<p>而对小数据的读取，HDFS更多的做法是吧小规模的随机读取操作进行合并后对读取顺序进行排序，这样可以在一定程度上按序读取，提高读取效率。因此可以说，HDFS更多地考虑对数据读取的批处理，而不是对单独命令的执行。</p>

<p>对于保证协调性来说，HDFS使用多种设计确保并提供了系统的灵活性，例如强调工作的协同与并发性，放松对一致性模型的要求等；还引入了写入锁的机制，对多个写入操作要求采用原子性，从而保证多用户在对同一文件进行写入操作时能够获得正确的写入行为。</p>

<p>对于HDFS架构来说，一个HDFS集群包括两大部分，即NameNode与DataNode。这样分配的主要作用是将管理与计算分离。一般来说，一个集群中会有一个NameNode与若干个DataNode共同工作。NameNode是集群的主服务器，主要是用于对HDFS中所有的文件及内容数据进行维护，并不断读取记录集群中DataNode主机情况与工作状态，并通过读取与写入镜像日志文件的方式进行存储。而DataNode在HDFS集群中担任任务具体执行角色，是集群的工作节点。文件被分成若干个大小相同的数据块，分别存储在若干个DataNode上，DataNode会定期向集群内NameNode发送自己的运行状态与存储内容，并根据NameNode发送的指令进行工作。NameNode负责接受客户端发送过来的信息，然后将文件存储位置信息发送给提交请求的客户端，由客户端直接与DataNode进行联系，从而进行部分文件的运算与操作。</p>

<p>HDFS使用Block对文件的存储进行操作。对于传统磁盘来说，磁盘都有默认的存储单元，通常使用的是数据定义中的最小存储单元。Block是HDFS的基本存储单元，默认大小是84M，这个大小远远大于一般文件系统的默认存储大小。这样做的好处有很多。第一，使用较大的存储块，即使将存储文件分隔在不同的存储介质中，也可以大大减少用户与节点之间的通信需求。一般的情况下，客户端与数据存储段可能分散在不同位置，彼此之间通过通信协议进行交流，较大的存储区域可以较少用户为获取数据存放位置信息而需要占用的资源，并且加大的存储块也方便用户更多次连续地进行读写操作。第二，采用较大尺寸的存储块，方便HDFS将更多的基本信息存放在节点内存中，这些基本信息包括文件名、存放的位置、存放地点等。</p>

<p>一般性文件的基本信息加载在内存之中，可以更好滴维护NameNode与DataNode之间的通信。</p>

<p>除此之外，采用Block对文件星星存储大大提供了文件灾难生存与恢复能力。HDFS还可以对已经存储的Block进行多副本备份，将每个Block至少复制3个相互独立的硬件上。这样做的好处是确保在发生硬件故障的时候，能够迅速地从其他意见中读取相应的文件数据。而具体复制到多少个独立硬件上也是可以设置的。</p>

<p>文件通过NameNode在系统中进行分配，DataNode负责对文件进行存储，并将本节点信息反馈给NameNode，从而完成文件存储的分配以及存储状态的获取。
HDFS是一种高度分布的多层结构布局。典型的HDFS集群是将数百个服务器同时部署在同一个集群之内，并且可以同时被来自同一或者不同地点的多个客户机访问。HDFS存储的目标有两个，即最大化数据可靠性与可用性以及最大化利用网络带宽资源。为了实现这两个目的，仅仅在多台机器上进行单一存储是不够的，不能预防硬件设备失误带来的损失。因此采用多副本存储复制，在多个节点之间重复分布存储数据副本，保证了即使有一个存储所需数据硬件设备损坏，也能够安全合理的获得所需要的数据。同时为扩大资源的利用带宽，对节点的选择则需要进行专门的研究与设计。</p>

<p>那么，NameNode如何选择具体的DataNode来存储数据，并能够充分利用现有的宽带那？在回答这个问题之前，首先需要对节点和宽带作出HDFS中的定义。在通过宽带网络进行数据交换与处理过程中，吧数据传输速度作为节点之间的衡量标准是自然并且可行的。但是问题在于，在实际工作中，宽带或者传输速率的影响的因素非常多，例如网络的稳定性、其他数据的传输宽带占用以及交换机的负载等等都能够对某个时间点的宽带起到决定性的作用。因此，并不能简单地吧传输速度作为节点距离的衡量标准。为了解决通过测量宽带不能够获得一个长期、准确、稳定速率的问题，HDFS采用了另一个简单的方法对宽带进行衡量。假设客户端即数据的获取点，根据数据存放的相对位置来衡量距离值。设定如下，距离值依次增加。
* 统一节点上的存储数据
* 同一机架上不同节点上的存储数据
* 同一数据中心不同机架上的存储数据
* 不同数据中心的节点</p>

<p>在确定数据距离值后，HDFS中NameNode节点即可根据定义的距离值对需要存储的数据进行切割和存储。同时，为了维护数据的稳定性与预防灾难，还需要将数据进行副本的备份操作。副本存放位置的选取是需要进行考虑和评价的。复制操作要对宽带及传输速率进行衡量，衡量的标准也是上文定义的距离值。如果想要要求更高的网络速度，那么将所有数据都存放在同一节点上市最为合适的，但是此种存放方法非常危险。如果你无法正确估量和测试硬件的稳定状态，一旦灾难发生，则可能无法避免数据损失。如果换另一种极端的方法考虑，将数据存放在不同数据中心不同节点中，那么，不同数据中心不同几点同时发生故障的可能性是非常低的。但是，由于客户端有可能需要多多个节点中的数据进行读写操作，因此，此种存放方法会在传输效率上大打折扣。</p>

<p>HDFS数据存放策略就是采用同节点与同机架并行的存储模式，制作成三个副本存储在运行客户端的当前节点上存放在第一个副本，第二个副本存在与第一个副本同一个机架上而非同一个节点，第三个副本放置的位置在另外一个机架上。</p>

<p>在HDFS上面，数据之间的交互是通过宽带网络进行的，而对于具体的交互程序，又分为输入和输出过程。</p>

<p>输入过程如下，客户端或者用户通过调用FileSystem对象的open()方法打开需要读取的文件，从而创建一个HDFS的读取对象实例。FileSystem通过远程协议调用NameNode获取文件处于前端的几个Block位置。并根据上下文定义客户端与DataNode距离值进行排序。NameNode将于客户端对应最近的那个文件起始Block地址返回给客户端，客户端根据地址创建一个FSDataInputStream开始对数据进行读取。FSDataInputStream根据返回的Block地址，链接到最近的DataNode上对数据Block开水读取，通过反复调用read()方法，以流的方式从DataNode中读取数据，当读到Block结尾的时候FSDataInputStream会关闭当前DataNode的链接，然后查找能够读取下一个Block的最好的DataNode。这些操作对客户端是透明的，客户端感觉到的是连续的流，也就是说读取的时候就开始查找下一个块所在的地址。读取完成，调用close方法，关闭FSDataInputStream。客户端在读取数据的过程中，只需要访问NameNode一次获得起始位置的Block，即可开始对数据进行持续而直接的读取。这样做的好处是HDFS能够服务大量的客户端。不同的数据传输是通过集群中不同的DataNode与客户端直接连接而成的。NameNode只需要提供一次查询基本信息的操作即可，而不需要提供任何数据服务工作，极大提高了HDFS的整体性能。</p>

<p>当客户端的读取操作发生错误是，客户端回想NameNode报告错误，并氢气球NameNode排除错误的DataNode后重新根据距离值排序，从而获得一个新的DataNode的读取路径。如果所有的DataNode都报告读取失败，那么任务读取失败。NameNode中存放的基本信息称为元数据，一般包括数据类型，大小，格式以及数据的存放形式。</p>

<p>与HDFS的输入类似，HDFS的输出也需要通过NameNode获得DataNode的基本信息，从而执行对应的输出过程。首先客户端通过调用create方法来向FileSystem请求创建文件。FileSystem向NameNode发出创建请求后，NameNode进行了若干步检查来确定需要创建的文件或目录是否已经存在，并且确认提出请求的客户端是否有权限创建相应的文件或目录。如果检查通过，NameNode会保存创建文件的请求并向客户端返回一个FSDataOutStream地址供客户端读出数据。而如果在NameNode中的检查没通过，则客户端会立即受到一个IOException，表示文件创建请求失败，任务被停止。客户端在受到FSDataOutputStream后调用write方法对数据进行输出操作。客户端会将数据分成若干个Block，然后在NameNode分配的FSDataOutputStream中到到具有最近距离的DataNode进行数据的输出。</p>

<p>对数据进行备份操作在数据被写到第一个DataNode完毕后立即开始，如果默认将数据进行三次副本备份的话，那么首选将数据写到第一个DataNode中。此后由第一个DataNode将数据复制到第二个DataNode，第二个DataNode会将数据写到第三个DataNode。全部数据写出结束后，FsDataOutputStream调用close方法，关闭写出流。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "hdfs.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
</body>
</html>
